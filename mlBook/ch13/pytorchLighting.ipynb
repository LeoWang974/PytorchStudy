{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-15T07:53:44.506627Z",
     "start_time": "2024-10-15T07:53:44.502394Z"
    }
   },
   "source": [
    "from pkg_resources import parse_version\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torchmetrics import __version__ as torchmetrics_version\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T07:53:44.524554Z",
     "start_time": "2024-10-15T07:53:44.512247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#构建lightning模型\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    #构造函数\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "    \n",
    "        # new PL attributes:\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        #添加了准确率属性，如self.train_acc = Accuracy()\n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    #简单的前向传播，并返回logits值用于训练、验证和测试步骤\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    #training_step,training_epoch_end等方法都是lightning独有的方法\n",
    "    #此方法定义了训练期间的单个前向传播，同时跟踪准确率和训练损失函数值，以供后续分析使用。\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)#返回logits\n",
    "        loss = nn.functional.cross_entropy(logits, y)#交叉熵\n",
    "        preds = torch.argmax(logits, dim=1)#argmax用于返回指定维度上最大值所在的索引。在这个上下文中，它被用来从模型的输出（logits）中选择最可能的类别。\n",
    "        self.train_acc.update(preds, y)     #此处计算了准确率，但没有保存\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    #该方法在每一轮训练结束时根据训练过程积累的准确率计算训练数据集的准确率\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "        self.train_acc.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        self.valid_acc.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    #在此处指定训练时使用的优化器\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer "
   ],
   "id": "fdf575d757c881f3",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T07:53:44.540727Z",
     "start_time": "2024-10-15T07:53:44.535509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#13.8.2为Lightning设置数据加载器\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=True) \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        mnist_all = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "\n",
    "        self.test = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)"
   ],
   "id": "ed629cad43391778",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T07:53:44.555043Z",
     "start_time": "2024-10-15T07:53:44.549323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1) \n",
    "mnist_dm = MnistDataModule()"
   ],
   "id": "a8abf2a02265ef25",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T07:53:48.032844Z",
     "start_time": "2024-10-15T07:53:47.958118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#13.8.3使用pytorch Lightning Trainer类训练模型\n",
    "mnistclassifier = MultiLayerPerceptron()\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ],
   "id": "bffb4a4db6d20365",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Support for `training_epoch_end` has been removed in v2.0.0. `MultiLayerPerceptron` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 7\u001B[0m\n\u001B[0;32m      3\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m [ModelCheckpoint(save_top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m'\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m)] \u001B[38;5;66;03m# save top 1 model\u001B[39;00m\n\u001B[0;32m      5\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[1;32m----> 7\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model\u001B[38;5;241m=\u001B[39mmnistclassifier, datamodule\u001B[38;5;241m=\u001B[39mmnist_dm)\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    540\u001B[0m )\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:931\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback_connector\u001B[38;5;241m.\u001B[39m_attach_model_callbacks()\n\u001B[0;32m    929\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback_connector\u001B[38;5;241m.\u001B[39m_attach_model_logging_functions()\n\u001B[1;32m--> 931\u001B[0m _verify_loop_configurations(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    933\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    934\u001B[0m \u001B[38;5;66;03m# SET UP THE TRAINER\u001B[39;00m\n\u001B[0;32m    935\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    936\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: setting up strategy environment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:36\u001B[0m, in \u001B[0;36m_verify_loop_configurations\u001B[1;34m(trainer)\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected: Trainer state fn must be set before validating loop configuration.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;241m==\u001B[39m TrainerFn\u001B[38;5;241m.\u001B[39mFITTING:\n\u001B[1;32m---> 36\u001B[0m     __verify_train_val_loop_configuration(trainer, model)\n\u001B[0;32m     37\u001B[0m     __verify_manual_optimization_support(trainer, model)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;241m==\u001B[39m TrainerFn\u001B[38;5;241m.\u001B[39mVALIDATING:\n",
      "File \u001B[1;32mD:\\software\\Anaconda\\envs\\pytorchStudy\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:77\u001B[0m, in \u001B[0;36m__verify_train_val_loop_configuration\u001B[1;34m(trainer, model)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# check legacy hooks are not present\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m---> 77\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m     78\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupport for `training_epoch_end` has been removed in v2.0.0. `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` implements this\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m instance attributes.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     81\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     82\u001B[0m     )\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupport for `validation_epoch_end` has been removed in v2.0.0. `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` implements this\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     86\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m method. You can use the `on_validation_epoch_end` hook instead. To access outputs, save them in-memory as\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     87\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m instance attributes.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     88\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     89\u001B[0m     )\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: Support for `training_epoch_end` has been removed in v2.0.0. `MultiLayerPerceptron` implements this method. You can use the `on_train_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520."
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
